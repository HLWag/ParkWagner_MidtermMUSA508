---
title: "Predicting Home Price in Miami"
author: "David Seunglee Park and Hannah Wagner"
date: "10/15/2020"
output: 
  html_document:
    code_folding: hide
    theme: paper
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
---

```{r initial, include=FALSE, message=FALSE, warning=FALSE}

library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot)
library(jtools)
library(mapview)
library(ggstance)
library(broom.mixed)
library(osmdata)
library(geosphere)
library(tidycensus)
library(stargazer)
library(kableExtra)
library(pander)
library(xtable)

options(tigris_class = "sf")
```

# 1. Introduction

Zillow’s housing market predictions, known as Zestimates, are valued for their nationwide coverage and general accuracy. For example, the nationwide median error for off-market homes is 7.5% and for on-market homes is 1.9%^[Zillow. 2020. "Zestimate." https://www.zillow.com/zestimate/]. However, when considering a specific city or region, the accuracy of the Zestimates could be improved by including locally-specific data in the prediction process. This analysis aims to build an accurate and generalizable hedonic model that predicts home prices for Miami by deconstructing overall home price into the value of constituent parts. Accurate models lead to only small differences between the predicted and observed values, and generalizable models accurately predict on new data and with comparable accuracy across various groups. By working to improve the accuracy and generalizability of the predictive model, we are ultimately striving to create a more useful decision-making tool. Such a model may be useful for local governments as they assess property taxes, for example.

## Modeling Strategy

To predict housing prices in the Miami area, we train a model using home prices from recent sales. Our model includes “features,” which are variables that are used to predict outcomes (in this case, home price). We use a hedonic model, which breaks home price into components that explain the cost: physical characteristics (e.g., living area), public services and amenities (e.g., distance to parks), and the spatial process of prices (e.g., prices for homes within neighborhoods cluster together). Our process uses data wrangling, exploratory analysis, feature engineering, feature selection, and model estimate and validation to produce a hedonic model that effectively predicts home prices for Miami and Miami Beach. Key challenges inherent to this process involve identifying and cleaning publicly available data for inclusion, investigating underlying spatial processes and trends, and selecting an effective set of features for inclusion in the model while avoiding choosing variables that are too closely correlated with each other. 

## Results
Our final model for predicting home prices includes the 20 independent variables shown in the Table below. Overall, the indicators used to evaluate the fit of the model (e.g., MAE) show that our model is not predicting well and is therefore not effective for use. The model was able to predict 85% of the variation in price, and most important features were internal property features, including lot size, actual square footage, whether the home is zoned for single family, and whether the home had an elevator or a dock. Generally, we think that our model's inability to predict expensive homes is the largest source of the error. This poor performance skews our overall MAE. According to the mapped results, we were unable to effectively account for the spatial variation in prices. The model predicted poorly in Miami Beach and along the mainland coast, which is where many of the expensive houses are located. We would recommend that Zillow make further improvements before employing the model for official use. The model has large errors and does not do a good job of predicting home prices. In order to improve this model, we suggest further exploring features that would better account for the spatial processes at play.

```{r variables, message=FALSE, warning=FALSE}
finalvars<- c('Lot Size', 'Number of Bedrooms', 'Number of Bathrooms', 'Home Age', 'Home Size', 'Presence of a Pool', 'Whether the Property is a Single Family Home', 'Presence of Luxury Features', 'Presence of an Elevator', 'Presence of a Dock', 'Estate Zoning', 'Number of Sexual Offenders or Predators within an Eighth Mile of the Home', 'Distance to the Nearest Park', 'Within 0.5 miles of a Metro Stop', 'Median Household Income', 'Percent of Population that is Hispanic', 'Percent of Population with a Bachelors Degree', 'Percent of Households that are Owned', 'Percent of Population that Commutes via Car', 'Percent of Households with a Household Income of $200,000 or more')

vartype<-c('Internal Characteristic', 'Internal Characteristic','Internal Characteristic','Internal Characteristic','Internal Characteristic','Internal Characteristic','Internal Characteristic','Internal Characteristic','Internal Characteristic','Internal Characteristic','Internal Characteristic', 'Amenities/Public Services', 'Amenities/Public Services', 'Amenities/Public Services', 'Spatial Structure', 'Spatial Structure', 'Spatial Structure', 'Spatial Structure', 'Spatial Structure', 'Spatial Structure')

datasource<-c('Home Data', 'Home Data', 'Home Data', 'Home Data', 'Home Data', 'Home Data', 'Home Data', 'Home Data', 'Home Data', 'Home Data', 'Home Data', 'Miami Dade County Open Data', 'OpenStreetMap', 'Miami Dade County Open Data', 'U.S. Census Bureau American Community Survey', 'U.S. Census Bureau American Community Survey', 'U.S. Census Bureau American Community Survey', 'U.S. Census Bureau American Community Survey', 'U.S. Census Bureau American Community Survey', 'U.S. Census Bureau American Community Survey')

varsdesc<- c(
  'Total Lot Size in Square Feet',
  'Total Number of Bedrooms in the Home',
  'Total Number of Bathrooms in the Home',
  'Age of the Home, as of 2020',
  'Total Size of the Home in Square Feet',
  'Whether or Not the Property Includes a Pool',
  'Whether the Property is a Single-Family Home or a Multi-Family Home',
  'Whether the Home Contains any Luxury Features',
  'Whether the Home Contains an Elevator',
  'Whether the Home Contains a Dock',
  'Whether the Home is Zoned as an Estate',
  'The Number of Sexual Offenders and Predators within an Eighth of a Mile of the Home',
  'Distance (in feet) to the nearest Park',
  'Whether the Home is Within a Half-Mile of a MetroMover or MetroRail Stop',
  'Median Household Income in the Census Tract',
  'Percent of Population in the Census Tract that is Hispanic',
  'Percent of Population in the Census Tract with a Bachelors Degree',
  'Percent of Households in the Census Tract that are Owned',
  'Percent of Population in the Census Tract that Commutes via Car',
  'Percent of Households in the Census Tract with a Household Income of $200,000 or more')

varstable<- data.frame(finalvars, datasource, varsdesc)

varstable%>%
  rename(Final_Variables = finalvars,
         #Variable_Type = vartype,
         Data_Source = datasource,
         Variable_Description = varsdesc)%>%
  kable(caption="Final Independent Variables Included in Home Price Prediction Model")%>%
  pack_rows("Internal Characteristic", 1, 11, label_row_css = "background-color: #25CB10") %>%
  pack_rows("Amenities/Public Services", 12,14, label_row_css = "background-color: #faf500")%>%
  pack_rows("Spatial Structure", 15, 20, label_row_css = "background-color: #FA7800")%>%
  kable_styling("striped", full_width = F)%>%
  scroll_box(height = "250px")
  
```


## Setup Code
To begin the analysis, this section loads libraries and options, specifies styling options for maps and plots, creates quantile break functions and quantile styling, and loads color palettes. This section also creates the "nearest neighbor" function, which calculates the average nearest neighbor distance from each home to its k nearest objects of interest. This calculation is useful for creating features that describe the relative amount of an amenity around each home. For example, the nearest neighbor function can tell us the average distance from each home to the closest 1, 2, 3, 4, or 5 parks, bars, and crime events. In our exploratory analysis, we determine which nearest neighbor features are most appropriate for inclusion in the model.

**Loading Libraries and Options**

```{r setup, include=TRUE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
	echo = TRUE)

library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot)
library(jtools)
library(mapview)
library(ggstance)
library(broom.mixed)
library(osmdata)
library(geosphere)
library(tidycensus)
library(stargazer)
library(kableExtra)
library(kableExtra)
library(stargazer)

options(tigris_class = "sf")
```

**Loading Themes for Map and Plots**

```{r Themes, message=FALSE, warning=FALSE}

mapTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 13,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2)
  )
}

plotTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle = element_text(face="italic"),
    plot.caption = element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.background = element_rect(fill = "lightskyblue1", color = "white"),
    strip.text = element_text(size=12),
    axis.title = element_text(size=12),
    axis.text = element_text(size=10),
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "italic"),
    legend.text = element_text(colour = "black", face = "italic"),
    strip.text.x = element_text(size = 14)
  )
}

palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")

qBr <- function(df, variable, rnd) {
  if (missing(rnd)) {
    as.character(quantile(round(df[[variable]],0),
                          c(.01,.2,.4,.6,.8), na.rm=T))
  } else if (rnd == FALSE | rnd == F) {
    as.character(formatC(quantile(df[[variable]]), digits = 3),
                 c(.01,.2,.4,.6,.8), na.rm=T)
  }
}

q5 <- function(variable) {as.factor(ntile(variable, 5))}
```

**Specifying Nearest Neighbor Function**

```{r NN Function, message=FALSE, warning=FALSE}
nn_function <- function(measureFrom,measureTo,k) {
  measureFrom_Matrix <- as.matrix(measureFrom)
  measureTo_Matrix <- as.matrix(measureTo)
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist
  output <-
    as.data.frame(nn) %>%
    rownames_to_column(var = "thisPoint") %>%
    gather(points, point_distance, V1:ncol(.)) %>%
    arrange(as.numeric(thisPoint)) %>%
    group_by(thisPoint) %>%
    summarize(pointDistance = mean(point_distance)) %>%
    arrange(as.numeric(thisPoint)) %>% 
    dplyr::select(-thisPoint) %>%
    pull()
  
  return(output)  
}
```

# 2. Data 
This analysis uses data from a variety of sources: we derived physical characteristics of each house from the underlying home data, gathered public services and amenities information from the Miami-Dade County Open Data Hub and OpenStreetMap, and incorporated population data from the U.S. Census Bureau’s 5-year American Community Survey.

## Data Gathering: Description & Code
We accessed data for the analysis via the Miami-Dade County Open Data Hub API and using the osmdata package to access OpenStreetMap variables of interest. Most of the data required manipulation and cleaning before being explored. This process included transformations in order to maximize the effectiveness of prediction for each variable. The code below presents the steps in the data gathering process. 

```{r}
text_tabl <-data.frame(
  Datasets = c("Miami Home Dataset", "Coastline Data", "Bars, Pubs, & Restaurant Data", "Sexual Offenders Data", "Miami Park Facilities Data","Miami-Dade Transit (MDT) Metromover and Metrorail stations", "2014-2018 ACS 5-year Estimates"),
  Source = c("Provided", "OpenStreetMap", "OpenStreetMap", "Miami-Dade County Open Data Hub", "Miami-Dade County Open Data Hub", "Miami-Dade County Open Data Hub", "U.S. Census Bureau")
)

kbl(text_tabl) %>%
  kable_paper(c("hover", "condensed"), full_width = F, html_font = "Montserrat") %>%
  kable_styling (bootstrap_options = "striped", "condensed", font_size = 10) %>%
  row_spec(0, bold = T, color = "white", background = "dodgerblue", font_size = 14)
```

**Loading Basemap**

```{r Basemap, message=FALSE, warning=FALSE, results='hide'}
# Loading Miami base map from OSM Data
miami.base <- 
  st_read("https://opendata.arcgis.com/datasets/5ece0745e24b4617a49f2e098df8117f_0.geojson") %>%
  filter(NAME == "MIAMI BEACH" | NAME == "MIAMI") %>%
  st_union()

xmin = st_bbox(miami.base)[[1]]
ymin = st_bbox(miami.base)[[2]]
xmax = st_bbox(miami.base)[[3]]  
ymax = st_bbox(miami.base)[[4]]
```

**Loading Home Data (Provided) and Neighborhood Data**

```{r Load Data, message=FALSE, warning=FALSE, results=FALSE, results='hide'}
MiamiProperties<-
  st_read("C:/Users/wagne/Documents/GitHub/ParkWagner_MidtermMUSA508/studentsData.geojson")%>%
  #st_read("/Users/davidseungleepark/Library/Mobile Documents/com~apple~CloudDocs/Fall 2020/cpln592/ParkWagner_MidtermMUSA508/studentsData.geojson") %>%
  
  mutate(pool = ifelse(str_detect((XF1), "Pool") | str_detect((XF2), "Pool") | str_detect((XF3), "Pool"), "Pool", "No Pool")) %>% 
  mutate(singlefamily = ifelse(str_detect(Zoning, "SINGLE FAMILY"), "Yes", "No")) %>%
  mutate(estates = ifelse(str_detect(Zoning, "ESTATES"), "Yes", "No")) %>%
  mutate(dock = ifelse(str_detect((XF1), "Dock") | str_detect((XF2), "Dock") | str_detect((XF3), "Dock"), "Dock", "No Dock")) %>%
  mutate(Age = (2020 - EffectiveYearBuilt),0) %>%
  mutate(luxury=ifelse(str_detect((XF1), "Luxury") | str_detect((XF2), "Luxury") | str_detect((XF3), "Luxury"), "Yes", "No")) %>%
  mutate(elevator=ifelse(str_detect((XF1), "Elevator") | str_detect((XF2), "Elevator") | str_detect((XF3), "Elevator"), "Yes", "No")) %>%
  mutate(Gazebo=ifelse(str_detect((XF1), "Gazebo") | str_detect((XF2), "Gazebo") | str_detect((XF3), "Gazebo"), "Yes", "No")) %>%
  mutate(BeachView=ifelse(str_detect((Legal1), "BEACH VIEW") | str_detect((Legal2), "BEACH VIEW") | str_detect((Legal3), "BEACH VIEW"), "Yes", "No")) %>%
  dplyr::select(-Land, -Year,-Bldg, -Total, -Assessed,  -saleDate, -saleType, -saleQual, -County.Senior, -County.LongTermSenior, -County.Other.Exempt, -Owner1, -Owner2, 
                -Mailing.Address, -Mailing.City, -Mailing.State, -Mailing.Zip, -Mailing.Country, -YearBuilt,
                -City.Senior, -City.LongTermSenior, -City.Other.Exempt, -Legal1, -Legal2, -Legal3, -Legal4, -Legal5, -Legal6, -XF1, -XF2, -XF3,
                -WVDB, -HEX, -GPAR, -County.2nd.HEX, -City.2nd.HEX, -MillCode, -Zoning, -Land.Use)

st_crs(MiamiProperties)

# Loading elementary school boundaries 
elementary.school.boundaries <- 
  st_read("https://opendata.arcgis.com/datasets/19f5d8dcd9714e6fbd9043ac7a50c6f6_0.geojson") %>%
  st_transform('ESRI:102658')

#Neighborhood Data
neighborhood<-
  st_read("https://opendata.arcgis.com/datasets/2f54a0cbd67046f2bd100fb735176e6c_0.geojson")%>%
  st_transform('ESRI:102658')%>%
  dplyr::select(LABEL)%>%
  rename(Neighborhood=LABEL)

muni_boundary<-
  st_read("https://opendata.arcgis.com/datasets/5ece0745e24b4617a49f2e098df8117f_0.geojson")%>%
  filter(NAME=="MIAMI BEACH")%>%
  st_transform('ESRI:102658')%>%
  dplyr::select(NAME)%>%
  rename(Neighborhood=NAME)

all_nhoods<-
  rbind(neighborhood,muni_boundary)

#Miami Beach Neighborhoods
MBAreas<-st_read("https://opendata.arcgis.com/datasets/a21e846f4e8e4d81ad3b75bc4f334516_0.geojson")%>%
  filter(FID>1)%>%
  filter(FID<130)%>%
  filter(LAND=="Y")%>%
  st_transform('ESRI:102658')%>%
  dplyr::select(FID)%>%
  rename(Neighborhood=FID)

miami.base_map<-
  miami.base%>%
  st_transform('ESRI:102658')

elem_map<-
  elementary.school.boundaries%>%
  st_crop(miami.base_map)%>%
  rename(elem_name=NAME)

all_nhoods_MB<-
  rbind(neighborhood,MBAreas)%>%
  st_crop(miami.base_map)
```

**Calculating Distance to Coastline; Calculating proximity to Bars, Pubs, and Restaurants; Sexual Offenses; Parks; and Public Transportation**

```{r Feature Engineering, message=FALSE, warning=FALSE, cache=TRUE, results='hide'}
## Calculate the distance to Coastline (this calculation has to be in WGS84)
Coastline<-opq(bbox = c(xmin, ymin, xmax, ymax)) %>% 
  add_osm_feature("natural", "coastline") %>%
  osmdata_sf()

### add to MiamiProperties and convert to miles
MiamiProperties <-
  MiamiProperties %>%
  mutate(CoastDist=(geosphere::dist2Line(p=st_coordinates(st_centroid(MiamiProperties)),
                                        line=st_coordinates(Coastline$osm_lines)[,1:2])*0.00062137)[,1])

##Convert Miami Data to Local Projection #st_transform('ESRI:102658')
MiamiProperties <-
  MiamiProperties%>%
  st_transform('ESRI:102658')

MiamiProperties<-
  MiamiProperties%>%
  mutate(milecoast=ifelse(CoastDist<1,"Yes","No"))%>%
  mutate(halfmilecoast=ifelse(CoastDist<0.5,"Yes","No"))

## Add Data on Bars, pubs, and restaurants
bars <- opq(bbox = c(xmin, ymin, xmax, ymax)) %>% 
  add_osm_feature(key = 'amenity', value = c("bar", "pub", "restaurant")) %>%
  osmdata_sf()

bars<-
  bars$osm_points%>%
  .[miami.base,]

bars<-
  bars%>%
  st_transform('ESRI:102658')
  
MiamiProperties<-
  MiamiProperties %>%
  mutate(
    bars_nn1= nn_function(st_coordinates(st_centroid(MiamiProperties)),st_coordinates(bars),1),
    bars_nn2= nn_function(st_coordinates(st_centroid(MiamiProperties)),st_coordinates(bars),2),
    bars_nn3= nn_function(st_coordinates(st_centroid(MiamiProperties)),st_coordinates(bars),3),
    bars_nn4= nn_function(st_coordinates(st_centroid(MiamiProperties)),st_coordinates(bars),4),
    bars_nn5= nn_function(st_coordinates(st_centroid(MiamiProperties)),st_coordinates(bars),5))

MiamiProperties$bars_Buffer =
  st_buffer(MiamiProperties, 660) %>%
  aggregate(mutate(dplyr::select(bars), counter = 1),., sum) %>%
  pull(counter)

MiamiProperties<-
  MiamiProperties%>%
  mutate(bars_Buffer = replace_na(bars_Buffer, 0))

## Add data on crime- Sexual Offenders and Predators within Miami-Dade County point data 
miami.sexualoffenders <-  
  st_read("https://opendata.arcgis.com/datasets/f8759d722aeb4198bfe7c4ad780604d2_0.geojson") %>%
  filter(CITY == "MIAMI" | CITY == "MIAMI BEACH" | CITY == "Miami" | CITY == "Miami Beach") %>%
  st_transform('ESRI:102658')

MiamiProperties$sexualoffenders_Buffer =
  st_buffer(MiamiProperties, 660) %>%
  aggregate(mutate(dplyr::select(miami.sexualoffenders), counter = 1),., sum) %>%
  pull(counter)

MiamiProperties<-
  MiamiProperties%>%
  mutate(sexualoffenders_Buffer = replace_na(sexualoffenders_Buffer, 0))

MiamiProperties <-
  MiamiProperties %>% 
  mutate(
    crime_nn1= nn_function(st_coordinates(st_centroid(MiamiProperties)),st_coordinates(miami.sexualoffenders),1),
    crime_nn2= nn_function(st_coordinates(st_centroid(MiamiProperties)),st_coordinates(miami.sexualoffenders),2),
    crime_nn3= nn_function(st_coordinates(st_centroid(MiamiProperties)),st_coordinates(miami.sexualoffenders),3),
    crime_nn4= nn_function(st_coordinates(st_centroid(MiamiProperties)),st_coordinates(miami.sexualoffenders),4),
    crime_nn5= nn_function(st_coordinates(st_centroid(MiamiProperties)),st_coordinates(miami.sexualoffenders),5))

## Add the data on Park Facilities
Parks<-st_read("https://opendata.arcgis.com/datasets/8c9528d3e1824db3b14ed53188a46291_0.geojson")%>%
st_transform('ESRI:102658')

MiamiProperties<-
  MiamiProperties %>% 
  mutate(
    parks_nn1= nn_function(st_coordinates(st_centroid(MiamiProperties)),st_coordinates(Parks),1),
    parks_nn2= nn_function(st_coordinates(st_centroid(MiamiProperties)),st_coordinates(Parks),2),
    parks_nn3= nn_function(st_coordinates(st_centroid(MiamiProperties)),st_coordinates(Parks),3),
    parks_nn4= nn_function(st_coordinates(st_centroid(MiamiProperties)),st_coordinates(Parks),4),
    parks_nn5= nn_function(st_coordinates(st_centroid(MiamiProperties)),st_coordinates(Parks),5))

MiamiProperties$parks.Buffer =
  st_buffer(MiamiProperties, 660) %>%
  aggregate(mutate(dplyr::select(Parks), counter = 1),., sum) %>%
  pull(counter)

MiamiProperties<-
  MiamiProperties%>%
  mutate(parks.Buffer = replace_na(parks.Buffer, 0))

#TOD or non-TOD; distance to transit stop?
metrorail_stop<-st_read("https://opendata.arcgis.com/datasets/ee3e2c45427e4c85b751d8ad57dd7b16_0.geojson")%>%
  st_transform('ESRI:102658')%>%
  dplyr::select(NAME)

metromover_stop<-st_read("https://opendata.arcgis.com/datasets/aec76104165c4e879b9b0203fa436dab_0.geojson")%>%
  st_transform('ESRI:102658')%>%
  dplyr::select(NAME)

metro_stops<-
  rbind(metromover_stop,metrorail_stop)

#Distance to metro_stop (Added a column for the distance to the nearest stop and a column for homes that are within 0.5 miles of a stop)
MiamiProperties<-
  MiamiProperties %>% 
  mutate(
    metro_nn1= nn_function(st_coordinates(st_centroid(MiamiProperties)),st_coordinates(metro_stops),1),
    Halfmile_metro=ifelse(metro_nn1<2640,"Halfmile_metro","Non_Halfmile_metro"))
                                                      
```

**Adding Neighborhood and Elementary School Names to Each Home**

```{r Neighborhood, message=FALSE, warning=FALSE, results='hide'}
#Add Elementary School Name to Each Property
elementary.school.clean<-
  elementary.school.boundaries%>%
  dplyr::select(NAME)%>%
  rename(elem_name=NAME)

MiamiProperties <- st_join(st_centroid(MiamiProperties), elementary.school.clean, left = TRUE)

#Add neighborhood name to each property
MiamiProperties <- st_join(st_centroid(MiamiProperties), all_nhoods, left = TRUE)

#Add variable for Miami or Miami Beach
MiamiProperties<-
  MiamiProperties%>%
  mutate(MiamiBeach=ifelse(Property.City=="Miami Beach","Yes","No"))
```

**Loading Census Data**

```{r Census data,message=FALSE, warning=FALSE, cache=TRUE, results='hide'}
## Load in census data
census_api_key("41e1c0d912341017fa6f36a5da061d3b23de335e", overwrite = TRUE)

selected_vars <- c("B02001_001E", # Estimate!!Total population by race -- ##let's double check that it's okay to use this as long as we justify it
                   "B02001_002E", # People describing themselves as "white alone"
                   "B02001_003E", # People describing themselves as "black" or "african-american" alone
                   "B15001_050E", # Females with bachelors degrees
                   "B15001_009E", # Males with bachelors degrees
                   "B19013_001E", # Median HH income
                   "B25058_001E", # Median rent
                   "B06012_002E", # Total poverty
                   "B08301_001E", # People who have means of transportation to work
                   "B08301_002E", # Total people who commute by car, truck, or van
                   "B08301_010E", # Total people who commute by public transportation"
                   "B03002_012E", # Estimate Total Hispanic or Latino by race
                   "B19326_001E", # Median income in past 12 months (inflation-adjusted)
                   "B07013_001E", # Total households
                   "B07013_002E", # Total owner-occupied households
                   "B07013_003E", # total renter-occupied households
                   "B25027_001E",
                   "B25027_010E",
                   "B25038_002E",
                   "B25038_003E",
                   "B25038_004E",
                   "B25038_005E",
                   "B19001_017E")

tracts18 <- 
  get_acs(geography = "tract", 
          variables = selected_vars, 
          year=2018, 
          state=12,
          county = 086,
          geometry=T, 
          output="wide") %>%
  st_transform('ESRI:102658') %>%
  rename(TotalPop = B02001_001E, 
         Whites = B02001_002E,
         Blacks = B02001_003E,
         FemaleBachelors = B15001_050E, 
         MaleBachelors = B15001_009E,
         MedHHInc = B19013_001E, 
         MedRent = B25058_001E,
         TotalPoverty = B06012_002E,
         TotalCommute = B08301_001E,
         CarCommute = B08301_002E,
         PubCommute = B08301_010E,
         TotalHispanic = B03002_012E,
         MedInc = B19326_001E,
         TotalHH = B07013_001E,
         OwnerHH = B07013_002E,
         RenterHH = B07013_003E,
         #TotalHH2 = B25027_001E,
         HHNoMort = B25027_010E,
         Own2017later = B25038_003E,
         Own201516 = B25038_004E,
         Own201014 = B25038_005E,
         HH200k = B19001_017E) %>%
  dplyr::select(-NAME, -starts_with("B0"), -starts_with("B1"), -starts_with("B2")) %>%
  mutate(pctWhite = (ifelse(TotalPop > 0, Whites / TotalPop,0))*100,
         pctBlack = (ifelse(TotalPop > 0, Blacks / TotalPop,0))*100,
         pctHis = (ifelse(TotalPop >0, TotalHispanic/TotalPop,0))*100,
         pctBlackorHis = (ifelse (TotalPop>0, (Blacks+TotalHispanic)/TotalPop,0)) *100,
         pctBachelors = (ifelse(TotalPop > 0, ((FemaleBachelors + MaleBachelors) / TotalPop),0)) *100,
         pctPoverty = (ifelse(TotalPop > 0, TotalPoverty / TotalPop, 0))*100,
         pctCarCommute = (ifelse(TotalCommute > 0, CarCommute / TotalCommute,0))*100,
         pctPubCommute = (ifelse(TotalCommute > 0, PubCommute / TotalCommute,0))*100,
         pctOwnerHH = (ifelse(TotalHH > 0, OwnerHH / TotalHH,0))*100,
         pctRenterHH = (ifelse(TotalHH > 0, RenterHH / TotalHH,0))*100,
         pctNoMortgage = (ifelse(TotalHH > 0, HHNoMort / TotalHH,0))*100,
         pctOwnerSince2010 = (ifelse(OwnerHH > 0, ((Own2017later + Own201516 + Own201014) / OwnerHH),0)) *100,
         pctHH200kOrMore = (ifelse(TotalHH > 0, (HH200k/ TotalHH),0))*100,
         year = "2018") %>%
  mutate(MedHHInc = replace_na(MedHHInc, 0),
         MedRent = replace_na(MedRent,0),
         pctBachelors= replace_na(pctBachelors,0),
         pctHis= replace_na(pctHis,0),
         pctOwnerHH= replace_na(pctOwnerHH,0),
         pctCarCommute= replace_na(pctCarCommute,0),
         pctHH200kOrMore= replace_na(pctHH200kOrMore,0)) %>%
  dplyr::select(-Whites, -Blacks, -FemaleBachelors, -MaleBachelors, -TotalPoverty, -CarCommute, -PubCommute, -TotalCommute, -TotalHispanic)

#merge the data into the MiamiProperties dataframe
MiamiProperties <-st_join((st_centroid(MiamiProperties)),tracts18, left =TRUE) %>%
                    mutate(MedHHInc = replace_na(MedHHInc, 0),
                           MedRent = replace_na(MedRent,0),
                           pctBachelors= replace_na(pctBachelors,0),
                           pctHis= replace_na(pctHis,0),
                           pctOwnerHH= replace_na(pctOwnerHH,0),
                           pctCarCommute= replace_na(pctCarCommute,0),
                           pctHH200kOrMore= replace_na(pctHH200kOrMore,0))
```

**Completing Feature Engineering and Filtering Out Homes Where Sales Prices Have Been Removed**

```{r remove toPredict, message=FALSE, warning=FALSE, results='hide'}
## create dataframe of homes to predict
MiamiPropertiesPred<-
  MiamiProperties %>%
  filter(toPredict == 1)

## create dataframe of rest of the homes
MiamiProperties<-
  MiamiProperties%>%
  filter(toPredict == 0)
```


## Variables
This is a table of all the engineered features that we considered including in the model. 

```{r}
engin_vars_table <-data.frame(
  Variable = c("CoastDist", "milecoast", "bars_nn1 ~ 5", "bars_Buffer", "crime_nn1 ~ 5","sexualoffenders_Buffer", "park_nn1 ~ 5", "parks.Buffer" , "metro_nn1", "Halfmile_metro","pctWhite", "pctBlack", "pctHis", "pctBlackorHis", "pctBachelors","pctPoverty", "pctCarCommute", "pctPubCommute", "pctOwnerHH", "pctRenterHH", "pctNoMortgage", "pctOwnerSince2010", "pctHH200kOrMore", "pool", "singlefamily", "estates", "dock", "Age", "luxury", "elevator", "BeachView"),
  Dataset = c("OpenStreetMap", "OpenStreetMap", "OpenStreetMap", "OpenStreetMap", "Miami-Dade County Open Data Hub", "Miami-Dade County Open Data Hub", "Miami-Dade County Open Data Hub", "Miami-Dade County Open Data Hub", "Miami-Dade County Open Data Hub", "Miami-Dade County Open Data Hub", "Miami-Dade County Open Data Hub", "U.S. Census Bureau", "U.S. Census Bureau", "U.S. Census Bureau", "U.S. Census Bureau", "U.S. Census Bureau", "U.S. Census Bureau", "U.S. Census Bureau", "U.S. Census Bureau", "U.S. Census Bureau", "U.S. Census Bureau", "U.S. Census Bureau", "U.S. Census Bureau", "Provide Miami Housing Data", "Provide Miami Housing Data", "Provide Miami Housing Data", "Provide Miami Housing Data", "Provide Miami Housing Data", "Provide Miami Housing Data", "Provide Miami Housing Data", "Provide Miami Housing Data"),
  Description = c("Distance to coast", "Units within a mile from the coast", "Average distances of 1-5 bars", "Number of bars within half mile from a housing units", "Average distances of 1-5 registered sexual offenders", "Number of sexual offenders within half mile from a housing unit", "Average distances of 1-5 parks", "Number of parks within half mile from the unit", "Distance to closest metrorail station", "Units within half mile from a metrorail stations", "Percentage of White population", "Percentage of Black population", "Percentage of Hispanic populaiton", "Percentage of Black and Hispanic population", "Percentage of population with at least Bachelor's degree", "Percentage of population below the poverty line", "Percentage of population communiting by car", "Percentage of population commuting by public transit", "Percentage of households in owner-occupied units", "Percentage of households in renter-occupied units", "Percentage of households with no mortgage", "Percentage of households who owned the house since 2010 or later", "Percentage of households with income of $200K or more", "Units with a pool", "Single Family zoning units", "Estates Zoning Units", "Units with a dock", "Age of the units", "Units with luxuary amenitiss", "Units with an elevator", "Units with a beach view"),
  "Data type" = c("continuous", "categorical", "continuous", "continuous", "continuous", "continuous", "continuous", "continuous", "continuous", "categorical", "continuous", "continuous", "continuous", "continuous", "continuous", "continuous", "continuous","continuous", "continuous", "continuous", "continuous", "continuous", "continous", "categorical", "categorical", "categorical", "categorical", "continous", "categorical", "categorical", "categorical"),
  "Final model" = c("No", "No", "No", "No", "No", "Yes", "Yes", "No", "No", "Yes", "No", "No", "Yes", "No", "Yes", "No", "Yes", "No", "Yes", "No", "No", "No", "Yes", "Yes", "Yes", "Yes", "Yes", "Yes", "Yes", "Yes", "No")
)

kbl(engin_vars_table) %>%
  kable_paper(c("striped", "hover"), html_font = "Montserrat", full_width = F) %>%
  kable_material() %>%
  kable_styling (bootstrap_options = "striped", "condensed", font_size = 11) %>%
  row_spec(0, bold = T, color = "white", background = "dodgerblue", font_size = 13)%>%
  column_spec(1, bold = T) %>%
  column_spec(2, width = "25%") %>%
  column_spec(3, width = "40%")
```

## Correlation:
As part of the exploratory analysis, we examined correlation to assist in identifying features that may be useful for predicting sales price. A correlation matrix helps us visualize correlation across numeric variables. In the figures, the darker colors imply stronger correlation. These plot helps to determine features that are correlated to sale prices (see sale prices row) and variables that are correlated with each other. The first figure depicts a correlation plot of variables derived from the U.S. Census Bureau. The second figure depicts the correlation between the numeric features included in the final model. We developed several correlation matrices with additional variables in order to help determine features to include in the final model.

```{r Correlation Matrix, message=FALSE, warning=FALSE}
#Correlation of Census Variables

censusVars <- 
  select(st_drop_geometry(MiamiProperties), SalePrice, MedHHInc, MedRent, pctWhite, pctBlack, pctHis, pctBlackorHis, 
          pctBachelors, pctPoverty, pctCarCommute, pctPubCommute, pctOwnerHH, pctRenterHH, pctNoMortgage, pctOwnerSince2010, pctHH200kOrMore) %>% na.omit()
ggcorrplot(
  round(cor(censusVars), 1), 
  p.mat = cor_pmat(censusVars),
  colors = c("#25CB10", "white", "#FA7800"),
  type="lower",
  insig = "blank") +  
  labs(title = "Correlation Across Census Variables",
       caption="Correlation Plot of Census Variables") 


#Correlation of Variables Included in Model
reg_final_vars <- 
  select(st_drop_geometry(MiamiProperties), SalePrice, LotSize, Bed, Bath, Age, ActualSqFt, parks_nn1, MedHHInc, pctHis, pctBachelors, pctOwnerHH, pctCarCommute, pctHH200kOrMore, sexualoffenders_Buffer) %>% na.omit()

ggcorrplot(
  round(cor(reg_final_vars), 1), 
  p.mat = cor_pmat(reg_final_vars),
  colors = c("#25CB10", "white", "#FA7800"),
  type="lower",
  insig = "blank") +  
  labs(title = "Correlation Across Final Numeric Features",
       caption="Correlation Plot of All Numeric Features Included in Final Model") 
```

The charts below plot sales price as a function of numeric features. The first figure is an example of a diagnostic tool used to select features for inclusion in the final model. The series of plots show the relationship between sale price and 6 crime variables for sexual offender and predators within the region (5 nearest neighbor variables and one buffer variable that counts the number of offenses within 1/8 mile of each home). By reviewing the slopes of the plotted lines, we selected the crime variable that was most highly correlated with Sale Prices to include in the final model. 

The second figure depicts four correlation scatterplots between continuous features and sale prices that suggest correlation. In all cases, the regression line slopes upward from left to right, meaning that on average, as the variable of interest (e.g., home size, median household income, distance to the nearest park, and household income greater than $200,000) increases, so does price. Correlation can also be described by the slope of the line. The greater the slope, the greater the feature's effect on price.

```{r Correlation scatterplots, message=FALSE, warning=FALSE}
#Correlation Scatterplots for all NN variable for Crime, as an example:
st_drop_geometry(MiamiProperties) %>% 
  dplyr::select(SalePrice, crime_nn1, crime_nn2, crime_nn3, crime_nn4, crime_nn5, sexualoffenders_Buffer) %>%
  gather(Variable, Value, -SalePrice) %>% 
  ggplot(aes(Value, SalePrice)) +
  geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
  facet_wrap(~Variable, ncol = 3, scales = "free") +
  labs(title = "Correlation between Sale Price and Crime Features",
       caption="Correlation Scatterplot Depicting the Relationship Between Sale Price and Crime Features") +
  plotTheme()+
  theme(strip.text.x = element_text(size = 10))


#Correlation Scatterplots for Variables of Interest
st_drop_geometry(MiamiProperties) %>% 
  mutate(Age = 2020 - EffectiveYearBuilt) %>%
  dplyr::select(SalePrice, LivingSqFt, parks_nn1, MedHHInc, pctHH200kOrMore) %>%
  #filter(SalePrice <= 1000000, Age < 500) %>%
  gather(Variable, Value, -SalePrice) %>% 
  ggplot(aes(Value, SalePrice)) +
  geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
  facet_wrap(~Variable, ncol = 3, scales = "free") +
  labs(title = "Price as a Function of Continuous Variables",
       caption="Correlation between Selected Continuous Variables and Sale Price") +
  plotTheme()+
  theme(strip.text.x = element_text(size = 10))
```

Some of the features included in the analysis are categorical rather than numeric (see Figure 5). Instead of using slope to calculate correlation, we evaluate the difference in mean price across each category. Differences in mean sales prices across categories suggest that the variable may be a good predictor of Sale Prices. 

```{r Categorical variable charts, message=FALSE, warning=FALSE}
st_drop_geometry(MiamiProperties) %>%
  dplyr::select(SalePrice, pool, elevator, luxury)%>%
  gather(Variable,Value, -SalePrice)%>%
  ggplot(aes(Value, SalePrice))+
  geom_bar(position="dodge",stat="summary", fun.y="mean")+
  facet_wrap(~Variable, ncol=3, scales="free")+
  labs(title = "Price as a Function of Categorical Variables",
       caption="Correlation between Selected Categorical Variables and Average Sale Price") +
  plotTheme()
```

The figure below depicts the spatial distribution of sale prices (or price per square foot) across Miami and Miami Beach. The map suggests that prices are clustered within the city, rather than randomly distributed. In other words, homes with a greater price per square foot (orange color) are grouped together (e.g., on the eastern coast of the mainland and within Miami Beach) and homes with a lower price per square foot (green color) are grouped together father inland.

```{r Sale price map, message=FALSE, warning=FALSE}
Miami.Plot<-
  MiamiProperties%>%
  mutate(PricePerSq=SalePrice/ActualSqFt)

ggplot()+
  geom_sf(data=all_nhoods_MB, fill="grey40")+
  geom_sf(data=Miami.Plot, aes(colour=q5(PricePerSq)),
          show.legend="point", size=.75)+
  scale_colour_manual(values=palette5,
                      labels=qBr(Miami.Plot, "PricePerSq"),
                      name="Quintile\nBreaks")+
  labs(title="Price Per Square Foot, Miami",
       caption="Price per Square Foot for Homes in Miami and Miami Beach") +
  mapTheme()
```

The maps below depict independent variables included in the final model. The first map shows the density of sexual offenders and predators in Miami and Miami Beach. The locations with the greatest density of sexual offenders and predators in the northern portions of Miami. The second map shows the locations of single-family and multi-family homes. Many single family homes are clustered along the shorline, with multi-family homes largely located further inland. The third maps shows the locations of County, Municipal, State, and Federal Park Facilities in the area. Parks are fairly evenly distributed throughout the area, with a slightly lesser density of parks further inland.  

```{r independent variable maps, message=FALSE, warning=FALSE}

sexual_offend_map<-
  miami.sexualoffenders%>%
  st_intersection(all_nhoods)

Parks_map<-
  Parks%>%
  st_intersection(all_nhoods_MB)

grid.arrange(ncol = 2,
             ggplot() + geom_sf(data = all_nhoods_MB, fill = "grey40") +
  stat_density2d(data = data.frame(st_coordinates(sexual_offend_map)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "#25CB10", high = "#FA7800", name = "Density") +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Locations of Sexual Offenders and Predators",
       caption="Density of Sexual Offenders in Miami and Miami Beach") +
  mapTheme(),
  ggplot()+
  geom_sf(data=all_nhoods_MB, fill="grey40")+
  geom_sf(data=Miami.Plot, aes(colour=singlefamily),
          show.legend="point", size=.75)+
  labs(title="Single vs. Multi Family Homes",
       caption="Figure 9. Single and Multi Family Homes in Miami and Miami Beach") +
  mapTheme())
  
ggplot()+
    geom_sf(data=all_nhoods_MB, fill="grey40")+
    geom_sf(data=Parks_map, color="lightgreen", show.legend="point", size=0.75)+
    labs(title="Park Locations",
       catption="Locations of Park Facilities in Miami and Miami Beach") +
    mapTheme()

```

# 3. Methods

## Model Building

After exploring and testing correlations of all the variables, we have started our model building process by looking at only at physical attributes of the housing units **(reg1)**. Then we added additional engineered variables to see how the model performs **(reg2)**. In **reg3**, we included almost all variables, even the ones we have determined to have strong collinearity, just to see how the model with too many variables that are collinear would perform. 

```{r Models, include= FALSE, results='hide'}
## model with physical attributes of the housing
reg1 <- lm(SalePrice ~ ., data = st_drop_geometry(MiamiProperties) %>% 
                  dplyr::select(SalePrice, LotSize, Bed, Bath, Age, ActualSqFt, elevator, dock, BeachView, pool, luxury, singlefamily, estates))
summary(reg1) %>%
xtable() %>%
kable()

## add addtional engineered variables
reg2 <- lm(SalePrice ~ ., data = st_drop_geometry(MiamiProperties) %>% 
                  dplyr::select(SalePrice, LotSize, Bed, Bath, Age, ActualSqFt, 
                                parks_nn1, MedRent, MedHHInc, pctBlackorHis, pctBachelors, pool, 
                                pctOwnerHH, crime_nn5, milecoast, pctNoMortgage, pctOwnerSince2010, pctCarCommute, pctPubCommute, 
                                pctPoverty, Halfmile_metro, CoastDist, pctHH200kOrMore, elevator, dock, luxury, BeachView))
summary(reg2)

## include almost all variables
reg3 <- lm(SalePrice ~ ., data = st_drop_geometry(MiamiProperties) %>% 
                  dplyr::select(SalePrice, LotSize, Bed, Bath, Age, 
                                ActualSqFt, parks_nn1, MedRent, MedHHInc, pctWhite, pctBlack, pctHis, pctBlackorHis, pctBachelors, pool, 
                                singlefamily, pctOwnerHH, pctRenterHH, crime_nn5, milecoast, pctNoMortgage, pctOwnerSince2010, pctCarCommute, 
                                pctPubCommute, pctPoverty, Halfmile_metro, metro_nn1, sexualoffenders_Buffer, milecoast, CoastDist, pctHH200kOrMore))
summary(reg3)
```

After our initial exploratory models, we used the **stepwise backward method** to find the statistically best model. The **stepwise backward function** drops each individual variable at a time to find a model with the least **AIC (Akaike information criterion)** which estimates the quality of each model relative to other models. the output of this process, **reg4** was the model with the smallest AIC value, indicating this output is the statistically best model. 

```{r, echo = FALSE, include=FALSE, results='hide'}
##stepwise backward function
step( lm(SalePrice ~ ., data = st_drop_geometry(MiamiProperties) %>% 
           dplyr::select(SalePrice, LotSize, Bed, Bath, Age, 
                         ActualSqFt, parks_nn1, MedRent, MedHHInc, pctWhite, pctBlack, pctHis, pctBlackorHis, pctBachelors, pool, 
                         singlefamily, pctOwnerHH, pctRenterHH, crime_nn5, milecoast, pctNoMortgage, pctOwnerSince2010, pctCarCommute, pctPubCommute, 
                         pctPoverty, Halfmile_metro, metro_nn1, sexualoffenders_Buffer, milecoast, CoastDist,pctHH200kOrMore, luxury, elevator, dock, estates)), direction="backward")
```

```{r, include =FALSE, echo = FALSE, results='hide'}
reg4 <- lm(SalePrice ~ ., data = st_drop_geometry(MiamiProperties) %>% 
                           dplyr::select(SalePrice, LotSize, Bed, Bath, Age, ActualSqFt, parks_nn1, MedRent, MedHHInc, pctWhite, pctBlack, pctHis , pctBachelors, pool, singlefamily, pctOwnerHH, 
                                         pctNoMortgage,  pctCarCommute, pctHH200kOrMore , luxury, elevator, dock , estates))
```

The variables with crime factors were not part of the output from the stepwise backward function. However, we made a judgment call that crime (proximity or density of sexual offenders as a proxy) is important factor in the predicting the price of a home. So we decided to make a model (**reg5**) that includes **"crime_nn5"**, which indicates the average distances of the 5 nearest sexual offenders, and another model (**reg6**) that includes **"sexualoffenders_Buffer"**, which indicates the number of sexual offenders in a half mile buffer of a home.

```{r, include =FALSE, echo = FALSE, results='hide'}
# reg5: add crim_nn5
reg5 <- lm(SalePrice ~ ., data = st_drop_geometry(MiamiProperties) %>% 
                    dplyr::select(SalePrice, LotSize, Bed, Bath, Age, ActualSqFt, parks_nn1, MedRent, MedHHInc, pctBlackorHis, pctBachelors, pool, 
                                  singlefamily, pctOwnerHH, pctCarCommute, pctHH200kOrMore, Halfmile_metro, luxury, elevator, dock, crime_nn5, BeachView, estates))
# reg6: add crime buffer
reg6 <- lm(SalePrice ~ ., data = st_drop_geometry(MiamiProperties) %>% 
             dplyr::select(SalePrice, LotSize, Bed, Bath, Age, ActualSqFt, parks_nn1, MedHHInc, pctHis, pctBachelors, pool, 
                            singlefamily, pctOwnerHH, pctCarCommute, pctHH200kOrMore, Halfmile_metro, luxury, elevator, dock, sexualoffenders_Buffer, estates))
```

```{r, echo = FALSE, warning = FALSE, results = 'asis'}
stargazer(reg1, reg2, reg3, reg4, reg5, reg6, type = "html")
```

Comparing the models we have created in the table above, we have decided that **reg6**, despite having a marginally smaller p-value than **reg4**, has the variables that we believe are critical for price prediction. 

### Final Model
```{r, final_model, echo=FALSE}
pander(reg6)
```


# 4. Results

To test our selected model, we randomly divided our master dataset, MiamiProperties, into separate training (60%) and test (40%) sets. The table shows the results of model.

```{r Training & Test Sets, message=FALSE, warning=FALSE, results='asis'}
# set random seed
set.seed(31337)

# get index for training sample
inTrain <- caret::createDataPartition(
  y = paste(MiamiProperties$Neighborhood),
  p = .60, list = FALSE)
# split data into training and test
Miami.training <- MiamiProperties[inTrain,] 
Miami.test     <- MiamiProperties[-inTrain,]  

#Regression
reg_final_train <- lm(SalePrice ~ ., data = st_drop_geometry(Miami.training) %>% 
                  dplyr::select(SalePrice, LotSize, Bed, Bath, Age, ActualSqFt, parks_nn1, MedHHInc, pctHis, pctBachelors, pool, 
                                singlefamily, pctOwnerHH, pctCarCommute, pctHH200kOrMore, Halfmile_metro, luxury, elevator, dock, sexualoffenders_Buffer, estates))


stargazer(reg_final_train, type = "html", main="Training Set Model Summary Results", caption="Model Summary Results for Training Dataset")
```

As an indicator of how well our model is performing, the values below show the mean average error of our model on the training and test set. We do think these error values are high. However, as discussed below, our model  has issues predicting higher priced homes, which skews our mean average error. Therefore, we don't think the mean average error is the best single indicator to validate the fit of our model.

```{r Predicted Prices as Function of Observed Prices, message=FALSE, warning=FALSE}
## predicting on new data
reg_final_predict <- predict(reg_final_train, newdata = Miami.test)

## Mean Square Error train and test
rmse.train <- caret::MAE(predict(reg_final_train), Miami.training$SalePrice)
rmse.test  <- caret::MAE(reg_final_predict, Miami.test$SalePrice)

cat("Train MAE: ", as.integer(rmse.train), " \n","Test MAE: ", as.integer(rmse.test))

```

The first plot below shows predicted home prices as a function of observed home prices in Miami for all homes. The bottom figure shows predicted home prices as a function of observed home prices in Miami divided between the test set and the training set. The orange line represents a scenario where the predicted home price perfectly matches the observed home price. The green line represents our model's prediction results. As the plots show, our model is systematically under predicting home prices (e.g., a home that is actually priced at $1 million is predicted to be approximately 600,000 dollars. The green line stays close to the perfect prediction line (orange) until homes with actual prices around 1 million dollars. At higher prices, our model performs worse. 

```{r , message=FALSE, warning=FALSE}
#Plotting accuracy metrics
preds.train <- data.frame(pred   = predict(reg_final_train),
                          actual = Miami.training$SalePrice,
                          source = "training data")
preds.test  <- data.frame(pred   = reg_final_predict,
                          actual = Miami.test$SalePrice,
                          source = "testing data")
preds <- rbind(preds.train, preds.test)

grid.arrange(nrow = 2,
ggplot(preds, aes(x = actual, y = pred)) +
  geom_point() +
  geom_smooth(method = "lm", color = "green") +
  geom_abline(color = "orange") +
  coord_equal() +
  theme_bw() +
  labs(title = "Comparing predictions to actual values for all homes",
       caption="Predicted Home Prices as a Function of Observed Home Prices",
       x = "Actual Value",
       y = "Predicted Value") +
  theme(),
ggplot(preds, aes(x = actual, y = pred, color = source)) +
  geom_point() +
  geom_smooth(method = "lm", color = "green") +
  geom_abline(color = "orange") +
  coord_equal() +
  theme_bw() +
  facet_wrap(~source, ncol = 2) +
  labs(title = "Comparing predictions to actual values for the test set and the training set",
       caption="Predicted Home Prices as a Function of Observed Home Prices for the Test Set and the Training Set",
       x = "Actual Value",
       y = "Predicted Value") +
  theme(
    legend.position = "none"))

```


The table below summarizes the mean and standard deviation MAE for our model. In the calculation of RMSE, errors are squared before they are averaged, which gives a relatively higher weight to larger errors. Because our model predicts poorly for highly priced houses, we are not surprised to see RMSE values that are greater than MAE values. In this case, MAE is a better metric to measure the accuracy of our model.

```{r MAE and MAPE for single test set, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}

########### CROSS- VALIDATION ##########

#Generalizability - cross validation
fitControl <- trainControl(method = "cv", number = 100, savePredictions = TRUE)
set.seed(825)

reg.cv <- 
  train(SalePrice ~ ., data = st_drop_geometry(MiamiProperties) %>% 
          dplyr::select(SalePrice, LotSize, Bed, Bath, Age, ActualSqFt, parks_nn1, MedHHInc, pctHis, pctBachelors, pool, 
                        singlefamily, pctOwnerHH, pctCarCommute, pctHH200kOrMore, Halfmile_metro, luxury, elevator, dock, sexualoffenders_Buffer, estates), 
        method = "lm", trControl = fitControl, na.action = na.pass)

reg.cv

reg.cv.results<-
reg.cv$results %>% 
  dplyr::select(-intercept)
  data.frame()
```


```{r , message=FALSE, warning=FALSE, echo=FALSE}
reg.cv.results%>%
  kable(caption = "Mean and Standard Deviation for the Mode")%>%
  kable_styling("striped", full_width = F)
```

The graphs below shows the results from the 100 folds. THe statistics cluster together which indicates that the model would be generalizable to new data. The relatively normal histogram also confirms that our model would generalize to new data with some errors.

```{r , message=FALSE, warning=FALSE, echo=FALSE}
#reg.cv$resample

reg.cv$resample %>% 
  pivot_longer(-Resample) %>% 
  mutate(name = as.factor(name)) %>% 
  ggplot(., aes(x = name, y = value, color = name)) +
  geom_jitter(width = 0.1) +
  facet_wrap(~name, ncol = 3, scales = "free") +
  theme_bw() +
  theme(
    legend.position = "none"
  )

reg.cv.dataframe<-
reg.cv$resample %>% 
  dplyr::select(MAE) %>%
  data.frame()

hist(reg.cv.dataframe$MAE, 
  main="Distribution of Mean Average Error from 100 Fold Test",
  xlab="Mean Average Error",
  ylab="Frequency")
```


```{r, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
# extract predictions from CV object
cv_preds <- reg.cv$pred
nrow(MiamiProperties)
nrow(cv_preds)

#Create dataset with "out of fold" predictions and original data
map_preds <- MiamiProperties %>% 
  rowid_to_column(var = "rowIndex") %>% 
  left_join(cv_preds, by = "rowIndex") %>% 
  mutate(SalePrice.AbsError = abs(pred - SalePrice)) %>% 
  cbind(st_coordinates(st_centroid((.))))

st_crs(map_preds) <- st_crs(all_nhoods)

# plot errors on a map
ggplot() +
  geom_sf(data = all_nhoods_MB, fill = "grey40") +
  geom_sf(data = map_preds, aes(colour = q5(SalePrice.AbsError)),
          show.legend = "point", size = 1) +
  scale_colour_manual(values = palette5,
                      labels=qBr(map_preds,"SalePrice.AbsError"),
                      name="Quintile\nBreaks") +
  labs(title="Absolute sale price errors on the OOF set",
       subtitle = "OOF = 'Out Of Fold'") +
  mapTheme()

#Start Ch4
k_nearest_neighbors = 20
#prices
coords <- st_coordinates(st_centroid(MiamiProperties)) 
# k nearest neighbors
neighborList <- knn2nb(knearneigh(coords, k_nearest_neighbors))
spatialWeights <- nb2listw(neighborList, style="W")
MiamiProperties$lagPrice <- lag.listw(spatialWeights, MiamiProperties$SalePrice)


#errors
Miami.test <-
  Miami.test %>%
  mutate(Regression = "Baseline Regression",
         SalePrice.Predict = ifelse((predict(reg_final_train, Miami.test)) < 0, mean(Miami.training$SalePrice), predict(reg_final_train, Miami.test)),
         SalePrice.Error = SalePrice.Predict - SalePrice,
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice),
         SalePrice.APE = (abs(SalePrice.Predict - SalePrice)) / abs(SalePrice.Predict))%>%
  mutate(SalePrice.AbsError = replace_na(SalePrice.AbsError, 0))%>%
  mutate(SalePrice.Error=replace_na(SalePrice.Error, 0))

coords.test <-  st_coordinates(st_centroid(Miami.test)) 
neighborList.test <- knn2nb(knearneigh(coords.test, k_nearest_neighbors))
spatialWeights.test <- nb2listw(neighborList.test, style="W")
Miami.test$lagPriceError <- lag.listw(spatialWeights.test, Miami.test$SalePrice.AbsError)

#summary(Miami.test$SalePrice.AbsError)
#summary(Miami.test$SalePrice.APE)
```

Overall, the mean absolute error for the test set is around 400,000 dollars and the mean absolute percentage error is over 100% (see below table). This indicates that our model does not perform well overall.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
#table of MAE and MAPE for single test set
Miami.test%>%
  st_drop_geometry()%>%
  summarize(Mean_Absolute_Error = mean(SalePrice.AbsError, na.rm = T),
            Mean_Absolute_Percentage_Error=mean(SalePrice.APE, na.rm=T)) %>%
  dplyr::select(Mean_Absolute_Error, Mean_Absolute_Percentage_Error)%>%
  kable(caption = "Mean Absolute Error and Mean Absolute Percentage Error for the Test Set") %>%
  kable_styling("striped", full_width = F)

```

The figure below shows a map of residuals for test set. Errors are both positive and negative, indicating that our model is both over and under predicting. 


```{r , message=FALSE, warning=FALSE, echo=FALSE}
#map of residuals for the test set (is the sale price error the residual?)
ggplot() +
  geom_sf(data = all_nhoods_MB, fill = "grey40") +
  geom_sf(data = Miami.test, aes(colour = q5(SalePrice.Error)),
          show.legend = "point", size = 1) +
  scale_colour_manual(values = palette5,
                      labels=qBr(Miami.test,"SalePrice.Error"),
                      name="Quintile\nBreaks") +
  labs(title="Sale price errors",
       caption="Map of residuals for the test set") +
  mapTheme()
```

The figures below shows the plot of spatial lag in errors for the test set. Spatial lag tells the average sale price of the 20 nearest neighbors. The plot of spatial lag of error tells us that as home price error increases, the errors or nearby home prices also increase. This correlation tells us that there is a spatial relationship that is unaccounted for in the model. Similarly, the Moran's I plot provides a secondary visual confirmation that spatial autocorrelation remains.

```{r , message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
##lag of price

Miami.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.AbsError))
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
grid.arrange(ncol=2,
ggplot(MiamiProperties, aes(x=lagPrice, y=SalePrice)) +
 geom_point(colour = "#FA7800") +
  geom_smooth(method = "lm", se = FALSE, colour = "#25CB10") +
  labs(title = "Price as a function of the spatial lag of price",
       x = "Spatial lag of price (Mean price of 20 nearest neighbors)",
       y = "Sale Price") +
  plotTheme(),
ggplot(Miami.test, aes(x=lagPriceError, y=SalePrice)) +
  geom_point(colour = "#FA7800") +
  geom_smooth(method = "lm", se = FALSE, colour = "#25CB10") +
  labs(title = "Error as a function of the spatial lag of price",
       x = "Spatial lag of errors (Mean error of 20 nearest neighbors)",
       y = "Sale Price") +
  plotTheme())

#Moran's I - A measure of spatial correlation
moranTest <- moran.mc(Miami.test$SalePrice.AbsError, 
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count",
       caption="Plot of Observed and Permuted Moran's I for the Test Set") +
  plotTheme()
```

```{r, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
#Errors by group - Neighborhood, Elementary School
##Neighborhood
nhood_sum <- Miami.test %>% 
  group_by(Neighborhood) %>%
  summarize(meanPrice = mean(SalePrice, na.rm = T),
            meanPrediction = mean(SalePrice.Predict, na.rm = T),
            meanMAE = mean(SalePrice.AbsError, na.rm = T))

nhood_sum %>% 
  st_drop_geometry %>%
  arrange(desc(meanMAE)) %>% 
  kable() %>% kable_styling()

map_preds_sum <- map_preds %>% 
  group_by(Neighborhood) %>% 
  summarise(meanMAE = mean(SalePrice.AbsError))

#I don't think we need these plots?
ggplot() +
  geom_sf(data = all_nhoods %>% 
            left_join(st_drop_geometry(map_preds_sum), by = "Neighborhood"),
          aes(fill = q5(meanMAE))) +
  scale_fill_manual(values = palette5,
                    labels=qBr(nhood_sum,"meanMAE"),
                    name="Quintile\nBreaks") +
  mapTheme() +
  labs(title="Absolute sale price errors on the OOF set by Neighborhood")
```

The table below shows the results of our attempt to account for spatial autocorrelation using a neighborhood feature. However, adding in the neighborhood effect did not greatly improve the results, likely due to a few very expensive homes that our model poorly predicts.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
reg.nhood <- lm(SalePrice ~ ., data = as.data.frame(Miami.training) %>% 
                  dplyr::select(Neighborhood, SalePrice, LotSize, Bed, Bath, Age, ActualSqFt, parks_nn1, MedHHInc, pctHis, pctBachelors, pool, 
                                singlefamily, pctOwnerHH, pctCarCommute, pctHH200kOrMore, Halfmile_metro, luxury, elevator, dock, sexualoffenders_Buffer, estates))


Miami.test.nhood <-
  Miami.test %>%
  mutate(Regression = "Neighborhood Effects",
         SalePrice.Predict = ifelse((predict(reg.nhood, Miami.test)) < 0, mean(Miami.training$SalePrice), predict(reg.nhood, Miami.test)),
         SalePrice.Error = SalePrice - SalePrice.Predict,
         SalePrice.AbsError = abs(SalePrice - SalePrice.Predict),
         SalePrice.APE = (abs(SalePrice - SalePrice.Predict)) / abs(SalePrice)) %>%
  mutate(SalePrice.AbsError = replace_na(SalePrice.AbsError, 0))%>%
  mutate(SalePrice.Error=replace_na(SalePrice.Error, 0))

bothRegressions <- 
  rbind(
    dplyr::select(Miami.test, starts_with("SalePrice"), Regression, Neighborhood) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)),
    dplyr::select(Miami.test.nhood, starts_with("SalePrice"), Regression, Neighborhood) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)))    


st_drop_geometry(bothRegressions) %>%
  gather(Variable, Value, -Regression, -Neighborhood) %>%
  filter(Variable == "SalePrice.AbsError" | Variable == "SalePrice.APE") %>%
  group_by(Regression, Variable) %>%
  summarize(meanValue = mean(Value, na.rm = T)) %>%
  spread(Variable, meanValue) %>%
  kable() %>%
  kable_styling("striped", full_width = F)

bothRegressions %>%
  dplyr::select(SalePrice.Predict, SalePrice, Regression) %>%
  ggplot(aes(SalePrice, SalePrice.Predict)) +
  geom_point() +
  stat_smooth(aes(SalePrice, SalePrice), 
              method = "lm", se = FALSE, size = 1, colour="#FA7800") + 
  stat_smooth(aes(SalePrice.Predict, SalePrice), 
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  facet_wrap(~Regression) +
  labs(title="Predicted sale price as a function of observed price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  plotTheme() + theme(plot.title = element_text(size = 18, colour = "black")) 

```


The map shows predicted sale prices for all homes. For the most part, the model predicts the most expensive home prices in Miami Beach and along the coast of the mainland. 

```{r map of predicted values, message=FALSE, warning=FALSE}
secret_data <- MiamiPropertiesPred
secret_preds <- predict(reg6, newdata = secret_data)
output_preds <- data.frame(prediction = secret_preds, Folio = secret_data$Folio, team_name = "Miami Heat Predictors")
#write.csv(output_preds, "Miami Heat Predictors.csv")

known_preds <- predict(reg6, newdata = MiamiProperties)
known_preds_geom<-st_sf(prediction = known_preds, Folio = MiamiProperties$Folio, geometry= MiamiProperties$geometry)

secret_preds_geom <- st_sf(prediction = secret_preds, Folio = secret_data$Folio, geometry= secret_data$geometry)

allpreds<-rbind(known_preds_geom, secret_preds_geom)

ggplot()+
  geom_sf(data = all_nhoods_MB, fill = "grey40") +
  geom_sf(data = allpreds, aes(colour = q5(prediction)),
          show.legend = "point", size = 1) +
  scale_colour_manual(values = palette5,
                      labels=qBr(allpreds,"prediction"),
                      name="Quintile\nBreaks") +
  labs(title="Predicted Home Prices",
       caption="Predicted Home Prices for All Homes") +
  mapTheme()
  
```


The figure below shows the mean absolute percentage error by neighborhood. This map shoes that some neighborhoods have a higher mean absolute percentage error (depicted in orange) than the majority of the neighborhoods (in green).

```{r MAPE by Neighborhood, message=FALSE, warning=FALSE}
#map MAPE by neighborhoods
st_drop_geometry(bothRegressions) %>%
  group_by(Regression, Neighborhood) %>%
  summarize(mean.MAPE = mean(SalePrice.APE, na.rm = T)) %>%
  ungroup() %>% 
  left_join(all_nhoods) %>%
  st_sf() %>%
  ggplot() + 
  geom_sf(aes(fill = mean.MAPE)) +
  geom_sf(data = bothRegressions, colour = "black", size = .5) +
  facet_wrap(~Regression) +
  scale_fill_gradient(low = palette5[1], high = palette5[5],
                      name = "MAPE") +
  labs(title = "Mean test set MAPE by neighborhood",
       caption="Mean Absolute Percentage Error by Neighborhood for both the Baseline Regression and Neighborhood Effects") +
  mapTheme()

```

As seen in the figure below, there are two neighborhoods (Coral Gate and Flora Park) where the mean absolute percentage error is very high when the mean price is very low. In addition, there are two neighborhoods (Belle Island and San Marco Island) where the mean price is very high while the mean absolute percentage error is very low. For most neighborhoods, the mean price is less than 3,000,000 and the mean absolute percentage error is less than ~4. Overall, the mean absolute percentage errors are higher than is desired.

```{r MAPE Scatterplot, message=FALSE, warning=FALSE}
#Plot MAPE by neighborhoods
st_drop_geometry(Miami.test) %>%
  group_by(Neighborhood) %>%
  summarize(mean.MAPE = mean(SalePrice.APE, na.rm = T),
            mean.Price=mean(SalePrice, na.rm=T)) %>%
  ggplot(aes(mean.Price, mean.MAPE)) +
  geom_point(color="blue")+
  geom_text(aes(label=ifelse(mean.MAPE>5, Neighborhood,"")), hjust=0.2, vjust=-0.3)+
  geom_text(aes(label=ifelse(mean.Price>3000000, Neighborhood, "")),hjust=0.5, vjust=0)+
  labs(title = "Mean Absolute Percentage Error as a Function of Mean Price by Neighborhood",
       caption="Mean Absolute Percentage Error and Mean Price by Neighborhood") +
  plotTheme()

```

## Generalizability by Race and Income

As shown in the tables below, this model is not generalizable by race or income. These tables provide the Mean Absolute Percent Error for different income groups and homes located in majority Hispanic census tracts. For homes in majority Hispanic census tracts, the model performs better for majority non-hispanic without neighborhood effects. With respect to income groups, our model performs better for homes located in high income census tracts than for homes located in low income tracts. 

```{r Race and Income Context, message=FALSE, warning=FALSE}
#Race Context and Income Context (from book)
tracts18 <- 
  tracts18%>%
  mutate(raceContext = ifelse(pctHis > 50, "Majority Hispanic", "Majority Non-Hispanic"),
         incomeContext = ifelse(MedInc > 32322, "High Income", "Low Income"))

grid.arrange(ncol = 2,
             ggplot() + geom_sf(data = na.omit(tracts18), aes(fill = raceContext)) +
               scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Race Context") +
               labs(title = "Race Context") +
               mapTheme() + theme(legend.position="bottom"), 
             ggplot() + geom_sf(data = na.omit(tracts18), aes(fill = incomeContext)) +
               scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Income Context") +
               labs(title = "Income Context") +
               mapTheme() + 
               theme(legend.position="bottom"))

st_join(st_centroid(bothRegressions), tracts18) %>% 
  group_by(Regression, raceContext) %>%
  summarize(mean.MAPE = scales::percent(mean(SalePrice.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(raceContext, mean.MAPE) %>%
  kable(caption = "Table X. Test set MAPE by neighborhood racial context") %>%
  kable_styling("striped", full_width = F)

st_join(st_centroid(bothRegressions), tracts18) %>% 
  filter(!is.na(incomeContext)) %>%
  group_by(Regression, incomeContext) %>%
  summarize(mean.MAPE = scales::percent(mean(SalePrice.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(incomeContext, mean.MAPE) %>%
  kable(caption = "Table XI. Test set MAPE by neighborhood income context") %>%
  kable_styling("striped", full_width = F)

```


# 5. Discussion 

Overall, the indicators used to evaluate the fit of the model (e.g., MAE) show that our model is not predicting well and is therefore not effective for use. However, additional evaluations show that the model performs fairly well for all homes less than 10 million dollars. We were unable to effectively engineer features to correctly model prices for very expensive homes. That being said, some of the more interesting variables included in the model were percent of the population with a bachelors degree, presence of an elevator or a dock, and whether the home is single family or multi-family. 

The model was able to predict 85% of the variation in price, and most important features were internal property features, including lot size, actual square footage, whether the home is zoned for single family, and whether the home had an elevator or a dock. Generally, we think that our model's inability to predict expensive homes is the largest source of the error. This poor performance skews our overall MAE. According to the mapped results, we were unable to effectively account for the spatial variation in prices. The model predicted poorly in Miami Beach and along the mainland coast, which is where many of the expensive houses are located. 

# 6. Conclusion

We would recommend that Zillow make further improvements before employing the model for official use. The model has large errors and does not do a good job of predicting home prices. In order to improve this model, we suggest further exploring features that would better account for the spatial processes at play. The goal of adding these features would be to account for spatial autocorrelation.

